<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
   <link rel="stylesheet" href="/libs/highlight/github.min.css">
   
  <link rel="stylesheet" href="/css/franklin.css">
  <link rel="stylesheet" href="/css/basic.css">
   <title>A tour of Monte Carlo methods for sampling probability densities</title>  
</head>
<body>
  <header>
	  <!--div class="blog-name"><a href="/">bb</a></div--->
  </header>

<!-- Content appended here -->

<div class="franklin-content">
<h1 id="a_tour_of_monte_carlo_methods_for_sampling_probability_densities"><a href="#a_tour_of_monte_carlo_methods_for_sampling_probability_densities">A tour of Monte Carlo methods for sampling probability densities</a></h1>
<p>&#39;Monte Carlo&#39; is an umbrella term for numerical methods that rely on random numbers to obtain some desired mathematical object or result.</p>
<p>One area where Monte Carlo methods are used a lot is in Bayesian statistics, where the main goal is to obtain expectations of arbitrary functions with respect to some complicated posterior probability &#40;target&#41; density \(\pi(\cdot)\). If the exact form of the target is available, a convenient way to approximate the desired expectations &#40;of a function \(f\), say&#41; is to draw a random sample of \(n\) points and compute the arithmetic average</p>
\[\mathbb{E}_\pi[f(X)] = \int f(x)\pi(x)dx \approx \frac{1}{n} \sum_{i=1}^n f(x_i)\]
<p>This simple Monte Carlo method conveys an important idea: we can efficiently approximate expectations under the target distribution given a random sample from that distribution.</p>
<p>However, the problem is that often we can not obtain a random sample from the target distribution. For instance, in all but the simplest Bayesian problems, the posterior can only be evaluated up to a normalizing constant that is an intractable high-dimensional integral.</p>
<p>In these notes I explore some Monte Carlo methods for sampling from such unnormalized densities. The rules of the game are simple: &#40;1&#41; we have a density function \(\pi(\cdot)\) taking values in \(\mathbb{R}^d\), &#40;2&#41; we can evaluate \(\pi\) up to a normalizing constant, but that&#39;s all and &#40;3&#41; we wish to obtain a random sample of \(n\) realizations \(x = (x_1, \dots, x_n) \in \mathbb{R}^d\) such that \(X \sim \pi\). Armed with such a sample, we can approximate expectations of functions of \(X\) with respect to \(\pi\)
<pre><code class="language-julia">using Plots, Random
Random.seed!(19081994)</code></pre>
<h2 id="importance_sampling"><a href="#importance_sampling">Importance sampling</a></h2>
<h2 id="rejection_sampling"><a href="#rejection_sampling">Rejection sampling</a></h2>
<h2 id="markov_chain_monte_carlo"><a href="#markov_chain_monte_carlo">Markov chain Monte Carlo</a></h2>
<p>Markov chain Monte Carlo &#40;MCMC&#41; is a very powerful and generic approach for sampling from complicated distributions. A vast literature exists on the subject, and many variants and implementations of the general methodology have been developed.</p>
<p>It is useful to partition the class of MCMC algorithms in two, based on whether they use information from the differential structure of the target or not.</p>
<h2 id="random-walk_metropolis-hastings_mcmc"><a href="#random-walk_metropolis-hastings_mcmc">Random-walk Metropolis-Hastings MCMC</a></h2>
<h2 id="hamiltonian_monte_carlo"><a href="#hamiltonian_monte_carlo">Hamiltonian Monte Carlo</a></h2>
<hr />
<p>Suppose we have some density \(\pi(x)\) for a random element \(X\) taking values \(x \in \mathbb{R}^d\), and further suppose we can evaluate it up to a normalizing constant</p>
<pre><code class="language-julia">π(x) = sum(x .^2) <= 1. && all(x .> 0.) ? x[1]^3*x[2] : zero(x[1])</code></pre>
<p>Note that <code>π&#40;x&#41;</code> coded up above is the <em>unnormalized version</em>. We assume we can evaluate the unnormalized density, but have no further analytical tools at our disposal &#40;e.g. we don&#39;t know how to integrate the density&#41;. The only thing we can do are queries like:</p>
<pre><code class="language-julia">π([0.9, 0.1])</code></pre>
<p>but that&#39;s it. Our goal is to obtain expectations of arbitrary functions \(f\) of \(X\) under the density \(\pi(x)\). For most random elements \(X\) this entails that we wish to evaluate integrals of the form:</p>
\[\mathbb{E}_\pi[f(x)] = \int f(x)\pi(x)dx \]
<p>For a low-dimensional problem like the one we have here, a first obvious thing to do is evaluate the density on a grid in \(\mathbb{R}^2\). We use this approach first to visualize the density:</p>
<pre><code class="language-julia">mn, mx, step = 0, 1, 0.01
plot(surface(mn:step:mx, mn:step:mx, (x,y)->π([x,y]), grid=false),
    contourf(mn:step:mx, mn:step:mx, (x,y)->π([x,y])), size=(800,300))
savefig("assets/unnormalized-density.svg")</code></pre>
<p><img src="assets/unnormalized-density.svg" alt="Visualization of the unnormalized density on a fine grid." /></p>
<p>But this approach can also be used to obtain integrals, in which case it is referred to as &#39;quadrature&#39;. For instance we can compute the normalizing constant as follows</p>
<pre><code class="language-julia">Z = 0.
for x₁=mn:step:mx, x₂=mn:step:mx
    global Z += π([x₁, x₂])*step^2
end
1/Z</code></pre>
<p>The full normalized density is actually \(\pi(x_1, x_2) = 24 x_1^3 x_2\) for \(x_1^2 + x_2^2 \le 1\) and \(x_1, x_2 > 0\), so we see &#40;surprise&#33;&#41; that numerical integration using quadrature works. We can use quadrature in this way to obtain expectations of any function of the random vector \(X\). For instance the mean of the first coordinate \(\mathbb{E}_\pi[X_1] = \int x_1 \pi(x) dx\)
<pre><code class="language-julia">E, Z = 0., 0.
for x₁=mn:step:mx, x₂=mn:step:mx
    p = π([x₁, x₂])*step^2
    global Z += p
    global E += x₁ * p
end
E/Z</code></pre>
<h2 id="some_more_1d_intuition"><a href="#some_more_1d_intuition">Some more 1D intuition</a></h2>
<p>Assume we have the following unnormalized density, which we can only evaluate</p>
<pre><code class="language-julia">π(x) = 0.3*exp(-(0.5-x)^2) + 0.7*exp(-(3-x)^2)</code></pre>
<p>It looks like this</p>
<pre><code class="language-julia">plot(x->π(x), xlim=(-2.5,6))</code></pre>
<p>a functio of interest to get the expectation of is</p>
<pre><code class="language-julia">f(x) = log(abs(x)+1)</code></pre>
<p>To compute an expectation, we can perform numerical integration</p>
<pre><code class="language-julia">mn, mx, step = -2.5, 6, 0.2
p = plot(x->π(x), xlim=(-2.5,6), color=:black, grid=false)
Z, E = 0., 0.
for x=mn:step:mx
    px = π(x)
    bar!([x], [px], bar_width=step, fillalpha=0, legend=false)
    global Z += step*px
    global E += step*f(x)*px
end
display(p)</code></pre>
<hr />
<p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em>
<div class="page-foot">
  <div class="copyright">
    &copy; Arthur Zwaenepoel. Last modified: April 02, 2020. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>.
  </div>
</div>

</div>
<!-- CONTENT ENDS HERE -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
        <script src="/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
