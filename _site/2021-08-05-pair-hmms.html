<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Pair hidden Markov models</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="/css/bb.css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Pair hidden Markov models</h1>
</header>
<p>Pair hidden Markov models (pair HMMs, or pHMMs) were introduced, as far as I can tell, by Durbin <em>et al.</em> (1999), in what I heard veterans call ‘the Durbin book’ (Biological Sequence Analysis, or BSA). These are beautiful probabilistic models that can be used for model-based pairwise alignment of sequences. As I was studying these, it seemed worthwhile to write down some notes, since the subject turned out to be somewhat more difficult than I thought (as a matter of fact, I thought I already understood how pHMMs worked, but that turned out to be only partly true when I actually needed one).</p>
<p>An ordinary hidden Markov model (HMM) is best understood by its probabilistic graphical model (PGM) representation</p>
<p><img src="img/hmm1.png" /></p>
<p>Recall that a PGM displays conditional dependence relationships among random variables. White circles usually denote unobserved (‘latent’) variables, whereas grey circles denote observed (‘clamped’) variables. In the above PGM, the white nodes display the ‘hidden’ states, which form a Markov chain, each variable being conditionally dependent on the preceding one. With each hidden state <span class="math inline">\(Z_i\)</span>, one random variable <span class="math inline">\(X_i\)</span> with law <span class="math inline">\(X_i|Z_i \sim f\)</span> conditionally dependent on the hidden state is associated, a realization of which constitutes the observed data. Clearly, such a probabilistic structure could be used to model a time series or a (biological) sequence (for instance). The probability law generating the observations conditional on the hidden states is often called the ‘emission model’. A classical example would be a model for some DNA sequence, where hidden states <span class="math inline">\(Z_i\)</span> could for instance correspond to whether or not a position <span class="math inline">\(i\)</span> is in (<span class="math inline">\(Z_i = 1\)</span>) or not in (<span class="math inline">\(Z_i = 0\)</span>) a <code>CpG</code> island. The emission model would correspond to a multinomial probability law with parameter <span class="math inline">\(\theta_i\)</span> depending on the hidden state. After specifying the transition probabilities <span class="math inline">\(p_{ij} = \Pr(Z_{i+1} = i|Z_i =j)\)</span> for hidden states and the emission model <span class="math inline">\(\theta_i|Z_i\)</span>, simulating from the HMM as a generative model is straightforward.</p>
<p>A first thing to stress is that a pair HMM is <em>au fond</em> just a HMM: a pair HMM has the same PGM as the one sketched above. There are however two aspects to pair HMMs and their use for probabilistic alignment which complicate things:</p>
<ol type="1">
<li>The Markov chain determining the hidden state transitions can be considerably more complicated than most textbook HMM examples.</li>
<li>We usually do not observe the data as emitted by the pair HMM.</li>
</ol>
<p>Honestly, the first aspect is what brought about some initial confusion, as I was used to looking at PGM diagrams, and got confused by the graphical display of the Markov chain (which is also depicted as a directed graph) <em>underneath</em> the hidden state part of the PGM, somehow trying to fit it in the PGM view. Specifically, the following two graphs depict two intimately related aspects of a pair-HMM</p>
<p><img src="img/hmm2.png" /></p>
<p>On the left side we of course have the same PGM as above, but where I put a box around what we have called the hidden nodes. On the right hand side, the Markov chain which governs the different states taken by the hidden nodes is displayed. So in this model, each hidden node takes a value from the set <span class="math inline">\(\{B,M,X,Y,E\}\)</span>, where transitions are allowed between neighboring states in the Markov chain.</p>
<p>The Markov chain over hidden states is determined by a transition probability matrix. In the above example this could for instance be</p>
<p><span class="math display">\[P = \begin{pmatrix}
0 &amp; 1-2\delta -\tau &amp; \delta &amp; \delta &amp; \tau \\ 
0 &amp; 1-2\delta- \tau &amp; \delta &amp; \delta &amp; \tau \\
0 &amp; 1-\epsilon-\tau &amp; \epsilon &amp; 0 &amp; \tau \\
0 &amp; 1-\epsilon-\tau &amp; 0 &amp; \epsilon &amp; \tau \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 
\end{pmatrix}\]</span></p>
<p>with states ordered <span class="math inline">\((B,M,X,Y,E)\)</span> this would correspond to the pHMM for a global alignment as defined in the Durbin book.</p>
</body>
</html>
